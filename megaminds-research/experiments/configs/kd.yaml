# Knowledge Distillation Training Configuration
# Student learns from Teacher with KD

# Model
model: 'yolov7_small'
nc: 20
teacher_model: 'yolov7_base'
teacher_weights: 'experiments/checkpoints/baseline/best.pt'

# Dataset
data: 'data/voc.yaml'
img_size: 640
batch_size: 16
workers: 4

# Training
epochs: 120
lr0: 0.01
lrf: 0.1
momentum: 0.937
weight_decay: 0.0005
warmup_epochs: 3
warmup_momentum: 0.8
warmup_bias_lr: 0.1

# Optimizer
optimizer: 'SGD'

# Scheduler
scheduler: 'cosine'

# Loss weights
box: 0.05
obj: 1.0
cls: 0.5

# Distillation parameters
alpha: 0.7  # weight for task loss (CE)
beta: 0.3   # weight for distillation loss (KL)
temperature: 3.0  # temperature for soft targets

# Augmentation
hsv_h: 0.015
hsv_s: 0.7
hsv_v: 0.4
degrees: 0.0
translate: 0.1
scale: 0.5
shear: 0.0
perspective: 0.0
flipud: 0.0
fliplr: 0.5
mosaic: 1.0
mixup: 0.0

# Checkpointing
save_dir: 'experiments/checkpoints/kd'
save_period: 10
resume: false
resume_path: null

# Logging
log_dir: 'experiments/logs/kd'
log_period: 10

# Validation
val_period: 5
conf_thres: 0.001
iou_thres: 0.6

# Device
device: 'cuda'
seed: 42

# Mixed precision
amp: true

# Early stopping
patience: 30
